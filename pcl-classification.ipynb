{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Import Statements**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, Dropout\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import RegexpTokenizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport seaborn as sns\nfrom tqdm import tqdm,tqdm_notebook\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\nfrom transformers import *\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-12-03T07:58:59.509202Z","iopub.execute_input":"2021-12-03T07:58:59.509948Z","iopub.status.idle":"2021-12-03T07:59:10.926455Z","shell.execute_reply.started":"2021-12-03T07:58:59.509843Z","shell.execute_reply":"2021-12-03T07:59:10.925285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Import Data**","metadata":{}},{"cell_type":"code","source":"training_csv = pd.read_csv('/kaggle/input/dontpatronizemepcl/dontpatronizeme_pcl.tsv', skiprows=3, sep='\\t', names=[\"paragraph_id\", \"keyword\", \"country_code\", \"paragraph\", \"label\"])\n#training_csv.columns = [\"paragraph_id\", \"keyword\", \"country_code\", \"paragraph\", \"label\"]\ndf = training_csv.drop([\"paragraph_id\", \"keyword\"], axis = 1, inplace = False)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-03T07:59:10.92888Z","iopub.execute_input":"2021-12-03T07:59:10.929437Z","iopub.status.idle":"2021-12-03T07:59:11.094227Z","shell.execute_reply.started":"2021-12-03T07:59:10.929385Z","shell.execute_reply":"2021-12-03T07:59:11.093212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory Data Analysis**","metadata":{}},{"cell_type":"code","source":"hist = df.hist(bins=5)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:44:33.463424Z","iopub.execute_input":"2021-12-02T07:44:33.463743Z","iopub.status.idle":"2021-12-02T07:44:33.784161Z","shell.execute_reply.started":"2021-12-02T07:44:33.4637Z","shell.execute_reply":"2021-12-02T07:44:33.783335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Processing**","metadata":{}},{"cell_type":"code","source":"stop = stopwords.words('english')\n\n# Use English stemmer.\nstemmer = SnowballStemmer(\"english\")\n\n# Define Lemmatizer\nlemmatizer = WordNetLemmatizer()\n\n#df = training_csv\n\ndef clean_text(col):\n    cleaned_col = col + \"_cleaned\"\n\n    #force text to lowercase\n    df[cleaned_col] = df[col].str.lower()\n    \n    #remove url from text\n    df[cleaned_col] = df[cleaned_col].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True).replace(r'[\\S]+\\.(net|com|org|info|edu|gov|uk|de|ca|jp|fr|au|us|ru|ch|it|nel|se|no|es|mil)[\\S]*\\s?','',regex = True)\n    \n    #remove numeric characters from text \n    df[cleaned_col] = df[cleaned_col].str.replace('\\d+', '')\n    \n    #remove punctuation from text\n    #df[cleaned_col] = df[cleaned_col].str.replace('[^\\w\\s]','')\n    \n    #remove stopwords\n    df[cleaned_col] = df[cleaned_col].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)]))\n    \n    #tokenized words\n    #token_col = col+\"_token\"\n    \n    #df[token_col] = df.apply(lambda row: nltk.word_tokenize(row[cleaned_col]), axis=1)\n    \n    #stem words\n    stem_col_name = col + \"_stemmed\"\n    #df[stem_col_name] = df[cleaned_col].str.split().apply(lambda x: [stemmer.stem(str(word)) for word in x])\n    \n    #lemmatize words\n    lem_col_name = col + \"_lemmatize\"\n    #df[lem_col_name] = df[cleaned_col].str.split().apply(lambda x: [lemmatizer.lemmatize(str(word)) for word in x])\n    \n    \nclean_text(\"paragraph\")\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-03T07:59:11.095561Z","iopub.execute_input":"2021-12-03T07:59:11.09583Z","iopub.status.idle":"2021-12-03T07:59:12.168477Z","shell.execute_reply.started":"2021-12-03T07:59:11.095799Z","shell.execute_reply":"2021-12-03T07:59:12.167094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"response\"] = df.apply(lambda x: 1 if x[\"label\"] > 1 else 0, axis = 1)\ndf = df.drop([\"paragraph\", \"label\"], axis = 1, inplace = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T08:00:43.239423Z","iopub.execute_input":"2021-12-03T08:00:43.239697Z","iopub.status.idle":"2021-12-03T08:00:43.350684Z","shell.execute_reply.started":"2021-12-03T08:00:43.239668Z","shell.execute_reply":"2021-12-03T08:00:43.349606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"UpSample=False\n\nif UpSample:\n\n    X,Y=RandomOverSampler(random_state=42).fit_resample(data_train.iloc[:,[0,1]],data_train.iloc[:,2])\n    data_train=pd.concat((pd.DataFrame(X),pd.DataFrame(Y)),axis=1)\n    data_train.columns=['country_code','paragraph_cleaned','response']\n    #data_val.columns=['country_code','paragraph_cleaned','response']\n    data_test.columns=['country_code','paragraph_cleaned','response']\n    data_train\n\nelse:\n    df.columns=['country_code','paragraph_cleaned','response']\n    #data_val.columns=['country_code','paragraph_cleaned','response'] \n    df.columns=['country_code','paragraph_cleaned','response']\n    df","metadata":{"execution":{"iopub.status.busy":"2021-12-03T08:00:44.914794Z","iopub.execute_input":"2021-12-03T08:00:44.91509Z","iopub.status.idle":"2021-12-03T08:00:44.921363Z","shell.execute_reply.started":"2021-12-03T08:00:44.915055Z","shell.execute_reply":"2021-12-03T08:00:44.920499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data,test_data = train_test_split(df, test_size=0.2)\ntrain_data = train_data.drop(train_data[train_data['response'] == 0].sample(frac=.2).index)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T08:00:54.394801Z","iopub.execute_input":"2021-12-03T08:00:54.395118Z","iopub.status.idle":"2021-12-03T08:00:54.411891Z","shell.execute_reply.started":"2021-12-03T08:00:54.395084Z","shell.execute_reply":"2021-12-03T08:00:54.410787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenizing Train Data \ntexts = train_data[\"paragraph_cleaned\"]\nlabels = train_data[\"response\"]\n\nmax_words = 10000\nmaxlen = 100\n #\ntokenizer = Tokenizer(num_words = max_words, oov_token = False) #oov_token = True, Unknown words are tokenized as 1\ntokenizer.fit_on_texts(texts) #Fit tokenizer on Train data only\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\n\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=maxlen)\nlabels = np.asarray(labels)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nx_train = data#[:training_samples]\ny_train = labels#[:training_samples]\n\n#Tokenizing Test Data\ntexts = test_data[\"paragraph_cleaned\"]\nlabels = test_data[\"response\"]\n\nsequences = tokenizer.texts_to_sequences(texts)\ndata = pad_sequences(sequences, maxlen=maxlen)\nlabels = np.asarray(labels)\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nx_test = data\ny_test = labels","metadata":{"execution":{"iopub.status.busy":"2021-12-03T08:00:57.356565Z","iopub.execute_input":"2021-12-03T08:00:57.357078Z","iopub.status.idle":"2021-12-03T08:00:57.835717Z","shell.execute_reply.started":"2021-12-03T08:00:57.357044Z","shell.execute_reply":"2021-12-03T08:00:57.834799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1-y_test.sum()/len(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:52:58.983222Z","iopub.execute_input":"2021-12-02T07:52:58.984053Z","iopub.status.idle":"2021-12-02T07:52:58.991278Z","shell.execute_reply.started":"2021-12-02T07:52:58.983996Z","shell.execute_reply":"2021-12-02T07:52:58.990456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = df[\"paragraph_cleaned\"]\nlabels = df[\"response\"]\n\nk = 4\nfold = 0\nnum_validation_samples = len(texts) // k\nvalidation_data = texts[num_validation_samples * fold:num_validation_samples * (fold + 1)]\n\nA = texts[:num_validation_samples * fold]\nB = texts[num_validation_samples * (fold + 1):] \ntraining_data = pd.concat([A,B])\n\nA = labels[:num_validation_samples * fold]\nB = labels[num_validation_samples * (fold + 1):] \ntraining_label = pd.concat([A,B])\n \nmax_words = 10000\nmaxlen = 100\n\ntokenizer = Tokenizer(num_words = max_words, oov_token = True) #oov_token = True, Unknown words are tokenized as 1\ntokenizer.fit_on_texts(training_data) #Fit tokenizer on Train data only\nsequences = tokenizer.texts_to_sequences(training_data)\n\n#print(sequences)\ndata = pad_sequences(sequences, maxlen=maxlen)\nY = np.asarray(training_label)\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\nlabels = Y[indices]\nx_train = data[indices]#[:training_samples]\ny_train = Y[indices]#[:training_samples]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:36:38.386128Z","iopub.execute_input":"2021-11-29T19:36:38.386497Z","iopub.status.idle":"2021-11-29T19:36:39.04339Z","shell.execute_reply.started":"2021-11-29T19:36:38.386455Z","shell.execute_reply":"2021-11-29T19:36:39.04243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kfold(k,df, model):\n    df = df.sample(frac=1, axis=1).reset_index(drop=True)\n    texts = df[\"paragraph_cleaned\"]\n    labels = df[\"response\"]\n    \n    num_validation_samples = len(texts) // k\n    validation_scores = []    \n    for fold in range(k):\n        #Selection of Train and Test data \n        validation_data = texts[num_validation_samples * fold:num_validation_samples * (fold + 1)]\n        \n        validation_label = labels[num_validation_samples * fold:num_validation_samples * (fold + 1)]\n        \n        A = texts[:num_validation_samples * fold]\n        B = texts[num_validation_samples * (fold + 1):] \n        training_data = pd.concat([A,B])\n        \n        A = labels[:num_validation_samples * fold]\n        B = labels[num_validation_samples * (fold + 1):] \n        training_label = pd.concat([A,B])\n        #Tokenizing Train Data \n        \n        max_words = 10000\n        maxlen = 100\n        \n        tokenizer = Tokenizer(num_words = max_words, oov_token = True) #oov_token = True, Unknown words are tokenized as 1\n        tokenizer.fit_on_texts(training_data) #Fit tokenizer on Train data only\n        sequences = tokenizer.texts_to_sequences(training_data)\n        \n        data = pad_sequences(sequences, maxlen=maxlen)\n        Y = np.asarray(training_label)\n        indices = np.arange(data.shape[0])\n        np.random.shuffle(indices)\n        x_train = data[indices]#[:training_samples]\n        y_train = Y[indices]#[:training_samples]\n        \n        #Tokenizing Test Data \n        \n        sequences = tokenizer.texts_to_sequences(validation_data)\n        data = pad_sequences(sequences, maxlen=maxlen)\n        Y = np.asarray(validation_label)\n        indices = np.arange(data.shape[0])\n        np.random.shuffle(indices)\n        x_test = data[indices]\n        y_test = Y[indices]\n\n        \n        print('Shape of train tensor:', x_train.shape)\n        print('Shape of test tensor:', x_test.shape)\n        \n        #model = get_model()\n        #model.train(training_data)\n        #validation_score = model.evaluate(validation_data)\n        #validation_scores.append(validation_score)\n         ","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:29:25.515619Z","iopub.execute_input":"2021-11-29T08:29:25.515887Z","iopub.status.idle":"2021-11-29T08:29:25.530686Z","shell.execute_reply.started":"2021-11-29T08:29:25.515858Z","shell.execute_reply":"2021-11-29T08:29:25.529766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = []\nk = 3\nkfold(k,df, model)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T08:29:27.853987Z","iopub.execute_input":"2021-11-29T08:29:27.854262Z","iopub.status.idle":"2021-11-29T08:29:29.761571Z","shell.execute_reply.started":"2021-11-29T08:29:27.854234Z","shell.execute_reply":"2021-11-29T08:29:29.760659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:54:12.896365Z","iopub.execute_input":"2021-12-02T07:54:12.897172Z","iopub.status.idle":"2021-12-02T07:54:12.906421Z","shell.execute_reply.started":"2021-12-02T07:54:12.897134Z","shell.execute_reply":"2021-12-02T07:54:12.905429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(1) Basic machine-learning approach**","metadata":{}},{"cell_type":"code","source":"embedding_dim = 16\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory1 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:20:17.359643Z","iopub.execute_input":"2021-11-30T18:20:17.360163Z","iopub.status.idle":"2021-11-30T18:20:33.47321Z","shell.execute_reply.started":"2021-11-30T18:20:17.36011Z","shell.execute_reply":"2021-11-30T18:20:33.472307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history1.history\nval_acc_values = history_dict['val_accuracy']\nval_acc_values","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:24:48.523047Z","iopub.execute_input":"2021-11-28T11:24:48.523301Z","iopub.status.idle":"2021-11-28T11:24:48.52982Z","shell.execute_reply.started":"2021-11-28T11:24:48.523272Z","shell.execute_reply":"2021-11-28T11:24:48.528881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history1.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:20:38.291603Z","iopub.execute_input":"2021-11-30T18:20:38.292101Z","iopub.status.idle":"2021-11-30T18:20:38.778603Z","shell.execute_reply.started":"2021-11-30T18:20:38.292065Z","shell.execute_reply":"2021-11-30T18:20:38.777403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(1.1) Adding dropout layer**","metadata":{}},{"cell_type":"code","source":"embedding_dim = 32\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory2 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:54:17.415932Z","iopub.execute_input":"2021-12-02T07:54:17.416467Z","iopub.status.idle":"2021-12-02T07:54:22.934595Z","shell.execute_reply.started":"2021-12-02T07:54:17.416422Z","shell.execute_reply":"2021-12-02T07:54:22.933261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history2.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:22:18.98315Z","iopub.execute_input":"2021-11-30T18:22:18.983449Z","iopub.status.idle":"2021-11-30T18:22:19.445797Z","shell.execute_reply.started":"2021-11-30T18:22:18.983413Z","shell.execute_reply":"2021-11-30T18:22:19.445178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history1.history\nloss_values1 = history_dict['loss']\nval_loss_values1 = history_dict['val_loss']\n\nhistory_dict = history2.history\nloss_values2 = history_dict['loss']\nval_loss_values2 = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values1, 'bo', label='Basic')\nplt.plot(epochs, loss_values2, 'bo', color='green', label='Basic+dropout')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, val_loss_values1, 'bo', label='Basic loss')\nplt.plot(epochs, val_loss_values2, 'bo', color='green', label='Basic+dropout')\nplt.title('Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:05.016428Z","iopub.execute_input":"2021-11-28T11:25:05.016728Z","iopub.status.idle":"2021-11-28T11:25:05.440744Z","shell.execute_reply.started":"2021-11-28T11:25:05.016688Z","shell.execute_reply":"2021-11-28T11:25:05.439927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(1.2) Weight Regularization**","metadata":{}},{"cell_type":"code","source":"from keras import regularizers\nembedding_dim = 32\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32,kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy',f1_m,precision_m, recall_m])\n\nhistory2 = model.fit(x_train,\n                    y_train,\n                    epochs=9,\n                    validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:54:30.950773Z","iopub.execute_input":"2021-12-02T07:54:30.951692Z","iopub.status.idle":"2021-12-02T07:54:42.439137Z","shell.execute_reply.started":"2021-12-02T07:54:30.951641Z","shell.execute_reply":"2021-12-02T07:54:42.438286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import regularizers\nembedding_dim = 32\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32,kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=[f1_m,precision_m])\n\nhistory2 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:51:23.079663Z","iopub.execute_input":"2021-11-30T18:51:23.079984Z","iopub.status.idle":"2021-11-30T18:51:48.861529Z","shell.execute_reply.started":"2021-11-30T18:51:23.079953Z","shell.execute_reply":"2021-11-30T18:51:48.86028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history2.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nf1_values = history_dict['f1_m']\nf1_val_values = history_dict['val_f1_m']\nplt.plot(epochs, f1_values, 'bo', label='Training acc')\nplt.plot(epochs, f1_val_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:55:12.347454Z","iopub.execute_input":"2021-12-02T07:55:12.347719Z","iopub.status.idle":"2021-12-02T07:55:12.381584Z","shell.execute_reply.started":"2021-12-02T07:55:12.347692Z","shell.execute_reply":"2021-12-02T07:55:12.380528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy, f1_score, precision, recall = model.evaluate(x_test, y_test, verbose = 0)\nprint('accuracy:', accuracy*100)\nprint('f1_score:', f1_score*100)\nprint('precision:', precision*100)\nprint('recall:', recall*100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\npredictions = model.predict(x_test)\nseq_predictions=np.transpose(predictions)[0]\nseq_predictions = list(map(lambda x: 0 if x<0.5 else 1, seq_predictions))\n\nprint(precision_score(y_test, seq_predictions))\nprint(recall_score(y_test, seq_predictions))\nprint(f1_score(y_test, seq_predictions))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:40:02.08044Z","iopub.execute_input":"2021-11-30T18:40:02.080798Z","iopub.status.idle":"2021-11-30T18:40:02.242881Z","shell.execute_reply.started":"2021-11-30T18:40:02.080765Z","shell.execute_reply":"2021-11-30T18:40:02.242132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(seq_predictions).T\n\ndf.to_csv(\"toutput.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:25:28.206014Z","iopub.execute_input":"2021-11-30T18:25:28.206554Z","iopub.status.idle":"2021-11-30T18:25:28.228979Z","shell.execute_reply.started":"2021-11-30T18:25:28.20652Z","shell.execute_reply":"2021-11-30T18:25:28.228381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history2.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:53:26.894572Z","iopub.execute_input":"2021-11-30T18:53:26.89491Z","iopub.status.idle":"2021-11-30T18:54:18.332406Z","shell.execute_reply.started":"2021-11-30T18:53:26.89487Z","shell.execute_reply":"2021-11-30T18:54:18.330197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history1.history\nloss_values1 = history_dict['loss']\nval_loss_values1 = history_dict['val_loss']\n\nhistory_dict = history2.history\nloss_values2 = history_dict['loss']\nval_loss_values2 = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values1, 'bo', label='Basic loss')\nplt.plot(epochs, loss_values2, 'bo', color='green', label='Basic+dropout loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, val_loss_values1, 'bo', label='Basic loss')\nplt.plot(epochs, val_loss_values2, 'bo', color='green', label='Basic+dropout loss')\nplt.title('Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.086931Z","iopub.status.idle":"2021-11-28T11:25:30.087428Z","shell.execute_reply.started":"2021-11-28T11:25:30.087139Z","shell.execute_reply":"2021-11-28T11:25:30.087163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(1.3) Add layer**","metadata":{}},{"cell_type":"code","source":"from keras import regularizers\nembedding_dim = 64\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory3 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:46:03.008446Z","iopub.execute_input":"2021-11-29T19:46:03.00875Z","iopub.status.idle":"2021-11-29T19:46:24.698626Z","shell.execute_reply.started":"2021-11-29T19:46:03.008718Z","shell.execute_reply":"2021-11-29T19:46:24.698023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import regularizers\nembedding_dim = 128\nmodel = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(128,kernel_regularizer=regularizers.l1(0.0005), activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\n#model.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory3 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.09055Z","iopub.status.idle":"2021-11-28T11:25:30.091009Z","shell.execute_reply.started":"2021-11-28T11:25:30.090752Z","shell.execute_reply":"2021-11-28T11:25:30.090776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#history_dict = history3.history\n#val_acc_values = history_dict['val_accuracy']\n#val_acc_values","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.092323Z","iopub.status.idle":"2021-11-28T11:25:30.092765Z","shell.execute_reply.started":"2021-11-28T11:25:30.092527Z","shell.execute_reply":"2021-11-28T11:25:30.09255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history3.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.09424Z","iopub.status.idle":"2021-11-28T11:25:30.09454Z","shell.execute_reply.started":"2021-11-28T11:25:30.094395Z","shell.execute_reply":"2021-11-28T11:25:30.094411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history1.history\nloss_values1 = history_dict['loss']\nval_loss_values1 = history_dict['val_loss']\n\nhistory_dict = history3.history\nloss_values2 = history_dict['loss']\nval_loss_values2 = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values1, 'bo', label='Basic loss')\nplt.plot(epochs, loss_values2, 'bo', color='green', label='Two layers loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, val_loss_values1, 'bo', label='Basic loss')\nplt.plot(epochs, val_loss_values2, 'bo', color='green', label='Two layers loss')\nplt.title('Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.095357Z","iopub.status.idle":"2021-11-28T11:25:30.095628Z","shell.execute_reply.started":"2021-11-28T11:25:30.095489Z","shell.execute_reply":"2021-11-28T11:25:30.095503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history2.history\n\ny_pred=model.predict(x_test)\n\ny_pred.map(lambda x: 0 if x<0.5 else 1, seq_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.09651Z","iopub.status.idle":"2021-11-28T11:25:30.096787Z","shell.execute_reply.started":"2021-11-28T11:25:30.096644Z","shell.execute_reply":"2021-11-28T11:25:30.096659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(x_test)\nseq_predictions=np.transpose(predictions)[0]\nseq_predictions = list(map(lambda x: 0 if x<0.5 else 1, seq_predictions))\n\nprint(precision_score(y_test, seq_predictions))\nprint(recall_score(y_test, seq_predictions))\nprint(f1_score(y_test, seq_predictions))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.097639Z","iopub.status.idle":"2021-11-28T11:25:30.097922Z","shell.execute_reply.started":"2021-11-28T11:25:30.09778Z","shell.execute_reply":"2021-11-28T11:25:30.097795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history2.history\nloss_values1 = history_dict['loss']\nval_loss_values1 = history_dict['val_loss']\n\nhistory_dict = history3.history\nloss_values2 = history_dict['loss']\nval_loss_values2 = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values1, 'bo', label='Basic with Regularization loss')\nplt.plot(epochs, loss_values2, 'bo', color='green', label='Two layers loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, val_loss_values1, 'bo', label='Basic with Regularization  loss')\nplt.plot(epochs, val_loss_values2, 'bo', color='green', label='Two layers loss')\nplt.title('Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.098628Z","iopub.status.idle":"2021-11-28T11:25:30.098903Z","shell.execute_reply.started":"2021-11-28T11:25:30.098758Z","shell.execute_reply":"2021-11-28T11:25:30.098774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history2.history\nloss_values1 = history_dict['accuracy']\nval_loss_values1 = history_dict['val_accuracy']\n\nhistory_dict = history3.history\nloss_values2 = history_dict['accuracy']\nval_loss_values2 = history_dict['val_accuracy']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values1, 'bo', label='Basic with Regularization')\nplt.plot(epochs, loss_values2, 'bo', color='green', label='Two layers')\nplt.title('Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_loss_values = history_dict['val_loss']\n\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, val_loss_values1, 'bo', label='Basic with Regularization')\nplt.plot(epochs, val_loss_values2, 'bo', color='green', label='Two layers')\nplt.title('Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.099823Z","iopub.status.idle":"2021-11-28T11:25:30.100109Z","shell.execute_reply.started":"2021-11-28T11:25:30.099966Z","shell.execute_reply":"2021-11-28T11:25:30.099981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\ny_pred1 = model.predict(x_test)\ny_pred = np.argmax(y_pred1, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.101028Z","iopub.status.idle":"2021-11-28T11:25:30.101348Z","shell.execute_reply.started":"2021-11-28T11:25:30.101163Z","shell.execute_reply":"2021-11-28T11:25:30.101178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\ncm = (y_test, y_pred)\n#print(classification_report(y_test, y_pred))\nprint(cm)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.102171Z","iopub.status.idle":"2021-11-28T11:25:30.102819Z","shell.execute_reply.started":"2021-11-28T11:25:30.102338Z","shell.execute_reply":"2021-11-28T11:25:30.102354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n...                               display_labels=clf.classes_)\n>>> disp.plot()\n<...>\n>>> plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.103893Z","iopub.status.idle":"2021-11-28T11:25:30.104164Z","shell.execute_reply.started":"2021-11-28T11:25:30.104021Z","shell.execute_reply":"2021-11-28T11:25:30.104035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred1","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.106747Z","iopub.status.idle":"2021-11-28T11:25:30.107074Z","shell.execute_reply.started":"2021-11-28T11:25:30.106909Z","shell.execute_reply":"2021-11-28T11:25:30.106931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print f1, precision, and recall scores\nprint(precision_score(y_test, y_pred1))\nprint(recall_score(y_test, y_pred1))\nprint(f1_score(y_test, y_pred1))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.108075Z","iopub.status.idle":"2021-11-28T11:25:30.108541Z","shell.execute_reply.started":"2021-11-28T11:25:30.108206Z","shell.execute_reply":"2021-11-28T11:25:30.10833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**(2) Conv1D**","metadata":{}},{"cell_type":"code","source":"max_features","metadata":{"execution":{"iopub.status.busy":"2021-11-29T20:09:48.405369Z","iopub.execute_input":"2021-11-29T20:09:48.4056Z","iopub.status.idle":"2021-11-29T20:09:48.435787Z","shell.execute_reply.started":"2021-11-29T20:09:48.405572Z","shell.execute_reply":"2021-11-29T20:09:48.434431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 32, input_length=maxlen))\nmodel.add(layers.Conv1D(16, 7,activation='relu'))\n#model.add(Dropout(0.5))\n#model.add(layers.MaxPooling1D(5))\n#model.add(layers.Conv1D(16, 7,activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(1))\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nhistory4 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:34:17.2622Z","iopub.execute_input":"2021-11-30T18:34:17.263457Z","iopub.status.idle":"2021-11-30T18:34:43.208465Z","shell.execute_reply.started":"2021-11-30T18:34:17.2634Z","shell.execute_reply":"2021-11-30T18:34:43.207447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history4.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T18:14:37.405336Z","iopub.execute_input":"2021-11-30T18:14:37.406283Z","iopub.status.idle":"2021-11-30T18:14:37.917278Z","shell.execute_reply.started":"2021-11-30T18:14:37.406232Z","shell.execute_reply":"2021-11-30T18:14:37.916436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 128, input_length=maxlen))\nmodel.add(layers.Conv1D(32, 7,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(layers.MaxPooling1D(5))\nmodel.add(layers.Conv1D(32, 7,activation='relu'))\n#model.add(Dropout(0.5))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(1))\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory4 = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    validation_data=(x_test,y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-28T12:06:06.192198Z","iopub.execute_input":"2021-11-28T12:06:06.192514Z","iopub.status.idle":"2021-11-28T12:07:28.368783Z","shell.execute_reply.started":"2021-11-28T12:06:06.19248Z","shell.execute_reply":"2021-11-28T12:07:28.367978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history4.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-28T12:09:38.104578Z","iopub.execute_input":"2021-11-28T12:09:38.105032Z","iopub.status.idle":"2021-11-28T12:09:38.55366Z","shell.execute_reply.started":"2021-11-28T12:09:38.104987Z","shell.execute_reply":"2021-11-28T12:09:38.552822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation Metric (F1)**","metadata":{}},{"cell_type":"code","source":"##Define evaluation metric\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.112403Z","iopub.status.idle":"2021-11-28T11:25:30.11268Z","shell.execute_reply.started":"2021-11-28T11:25:30.112539Z","shell.execute_reply":"2021-11-28T11:25:30.112553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#########################","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.11481Z","iopub.status.idle":"2021-11-28T11:25:30.115086Z","shell.execute_reply.started":"2021-11-28T11:25:30.114946Z","shell.execute_reply":"2021-11-28T11:25:30.114961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ignore from here down**","metadata":{}},{"cell_type":"code","source":"#########################","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.115643Z","iopub.status.idle":"2021-11-28T11:25:30.115921Z","shell.execute_reply.started":"2021-11-28T11:25:30.11577Z","shell.execute_reply":"2021-11-28T11:25:30.115784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Building**","metadata":{}},{"cell_type":"code","source":"max_seq_length=0\nfor i in tqdm(range(data_train.shape[0])):\n    max_seq_length=max(len(data_train.iloc[i,1].split()),max_seq_length)\nmax_seq_length=max_seq_length+2     \nprint(max_seq_length)\n\n# output : \n# 960","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.117192Z","iopub.status.idle":"2021-11-28T11:25:30.117923Z","shell.execute_reply.started":"2021-11-28T11:25:30.11774Z","shell.execute_reply":"2021-11-28T11:25:30.117761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_examples_to_features(sentences, label_list, max_seq_length, tokenizer):\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for index in tqdm_notebook(range(len(sentences)),desc=\"Converting examples to features\"):\n        sentence = sentences[index] #example.text_a.split(' ')\n        if sentence=='':\n            sentence=\" \"\n        input_id = tokenizer.encode_plus(sentence,max_length=max_seq_length,pad_to_max_length=True)['input_ids']\n        input_mask = tokenizer.encode_plus(sentence,max_length=max_seq_length,pad_to_max_length=True)['attention_mask']\n        segment_id = tokenizer.encode_plus(sentence,max_length=max_seq_length,pad_to_max_length=True)['token_type_ids']\n        label = label_list[index]\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels)\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.118671Z","iopub.status.idle":"2021-11-28T11:25:30.119367Z","shell.execute_reply.started":"2021-11-28T11:25:30.119157Z","shell.execute_reply":"2021-11-28T11:25:30.119178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer=RobertaTokenizer.from_pretrained('roberta-base')\n# #tokenizer=AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n\n# data_train=data_train.reset_index(drop=True)\n# #data_val=data_val.reset_index(drop=True)\n\n# (train_input_ids, train_input_masks, train_segment_ids, train_labels \n# ) = convert_examples_to_features(data_train['paragraph_cleaned'],data_train['response'],max_seq_length,tokenizer)\n\n# # (val_input_ids, val_input_masks, val_segment_ids, val_labels \n# # ) = convert_examples_to_features(data_val['text'],data_val['class'],max_seq_length,tokenizer)\n\n# (test_input_ids, test_input_masks, test_segment_ids, test_labels \n# ) = convert_examples_to_features(data_test['paragraph_cleaned'],np.ones(data_test.shape[0]),max_seq_length,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-11-28T11:25:30.120457Z","iopub.status.idle":"2021-11-28T11:25:30.120964Z","shell.execute_reply.started":"2021-11-28T11:25:30.120803Z","shell.execute_reply":"2021-11-28T11:25:30.120821Z"},"trusted":true},"execution_count":null,"outputs":[]}]}